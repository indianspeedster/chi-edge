{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using edge devices for CPU-based inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models are most often trained in the “cloud”, on\n",
    "powerful centralized servers with specialized resources (like GPU\n",
    "acceleration) for training machine learning models.\n",
    "\n",
    "However, for a variety of reasons including privacy, latency, and\n",
    "network connectivity or bandwidth constraints, it is often preferable to\n",
    "*use* these models (i.e. do inference) at “edge” devices located\n",
    "wherever the input data is/where the model’s prediction is going to be\n",
    "used.\n",
    "\n",
    "These edge devices are less powerful and typically lack any special\n",
    "acceleration, so the inference time (the time from when the input is fed\n",
    "to the model, until the model outputs its prediction) may not be as fast\n",
    "as it would be on a cloud server - but we avoid having to send the input\n",
    "data to the cloud and then sending the prediction back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes you already have a “lease” available for a device\n",
    "on the CHI@Edge testbed. Then, it will show you how to:\n",
    "\n",
    "-   launch a “container” on that device\n",
    "-   attach an IP address to the container, so that you can access it\n",
    "    over SSH\n",
    "-   transfer files to and from the container\n",
    "-   use a pre-trained image classification model to do inference on the\n",
    "    edge device\n",
    "-   delete the container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch a container on an edge device\n",
    "\n",
    "We will start by preparing our environment in this notebook, then\n",
    "launching a container on an edge device using our pre-existing lease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load some required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chi\n",
    "from chi import container\n",
    "from chi import lease\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indicate that we’re going to use the CHI@Edge site. We also need to\n",
    "specify the name of the Chameleon “project” that this experiment is part\n",
    "of. This can be found using the `os.getenv('OS_PROJECT_NAME')`. The\n",
    "project name will have the format “CHI-XXXXXX”, where the last part is a\n",
    "6-digit number. You can also find it on your [user\n",
    "dashboard](https://chameleoncloud.org/user/dashboard/).\n",
    "\n",
    "In the cell below, replace the project ID with your *own* project ID,\n",
    "then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = os.getenv('OS_PROJECT_NAME') #\"CHI-XXXXXX\" format; Change this if needed\n",
    "chi.use_site(\"CHI@Edge\")\n",
    "chi.set(\"project_name\", PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll specify the lease ID. This notebook assumes you already have\n",
    "a “lease” for a device on CHI@Edge. To get the ID of this lease,\n",
    "\n",
    "-   Vist the CHI@Edge [“reservations”\n",
    "    page](https://chi.edge.chameleoncloud.org/project/leases/).\n",
    "-   Click on the lease name.\n",
    "-   On the following page, look for the value next to the word “Id” in\n",
    "    the “Lease” section.\n",
    "\n",
    "Fill in the lease ID inside the quotation marks in the following cell,\n",
    "then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lease_id =\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to launch a container!\n",
    "\n",
    "-   **Container** : A container is like a logical “box” that holds\n",
    "    everything needed to run an application. It includes the application\n",
    "    itself, along with all the necessary prerequisite software, files,\n",
    "    and settings it needs to work properly.\n",
    "-   **Image** : An image is like a pre-packaged “starting point” for a\n",
    "    container. On CHI@Edge, we can use any image that is built for the\n",
    "    ARM64 architecture - e.g. anything on [this\n",
    "    list](https://hub.docker.com/search?type=image&architecture=arm64&q=).\n",
    "    In this example, we’re going to run a machine learning application\n",
    "    written in Python, so we will use the `python:3.9-slim` image as a\n",
    "    starting point for our container. This is a lightweight installation\n",
    "    of the Debian Linux operating system with Python pre-installed.\n",
    "\n",
    "When we create the container, we could also specify some additional\n",
    "arguments:\n",
    "\n",
    "-   `workdir`: the “working directory” - location in the container’s\n",
    "    filesystem from which any commands we specify will run.\n",
    "-   `exposed_ports`: if we run any applications inside the container\n",
    "    that need to accept incoming requests from a network, we will need\n",
    "    to export a “port” number for those incoming requests. Any requests\n",
    "    to that port number will be forwarded to this container.\n",
    "-   `command`: if we want to execute a specific command immediately on\n",
    "    starting the container, we can specify that as well.\n",
    "\n",
    "For this particular experiment, we’ll specify that port 22 - which is\n",
    "used for SSH access - should be exposed.\n",
    "\n",
    "Also, since we do not specify a `command` to run, we will further\n",
    "specify `interactive = True` - that it should open an interactive Python\n",
    "session - otherwise the container will immediately stop after it is\n",
    "started, because it has no “work” to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we’ll specify the name for our container - we’ll include our\n",
    "username and the experiment name in the container name, so that it will\n",
    "be easy to identify our container in the CHI@Edge web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.environ.get(\"USER\")\n",
    "expname = \"edge-cpu\"\n",
    "# set a name for the container\n",
    "# Note that underscore characters _ are not allowed - we replace each _ with a -\n",
    "container_name = f\"{username}-{expname}\".replace(\"_\",\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can create the container!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    my_container = container.create_container(\n",
    "        container_name,\n",
    "        image=\"python:3.9-slim\",\n",
    "        reservation_id=lease.get_device_reservation(lease_id),\n",
    "        interactive=True,\n",
    "        exposed_ports=[22],\n",
    "        platform_version=2,\n",
    "    )\n",
    "except RuntimeError as ex:\n",
    "    print(ex)\n",
    "    print(f\"Please stop and/or delete {container_name} and try again\")\n",
    "else:\n",
    "    print(f\"Successfully created container: {container_name}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell waits for the container to be active - when it is, it will\n",
    "print some output related to the container state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait until container is ready to use\n",
    "container.wait_for_active(my_container.uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the container is created, you should be able to see it and monitor\n",
    "its status on the [CHI@Edge web\n",
    "interface](https://chi.edge.chameleoncloud.org/project/container/containers).\n",
    "(If there was any problem while creating the container, you can also\n",
    "delete the container from that interface, in order to be able to try\n",
    "again.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach an address and access your container over SSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with a conventional “server” on Chameleon, we can attach an\n",
    "address to our container, then use SSH to access its terminal.\n",
    "\n",
    "First, we’ll attach an address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_ip = container.associate_floating_ip(my_container.uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to install an SSH server on the container - it is not\n",
    "pre-installed on the image we selected. We can use the\n",
    "`container.execute()` function to run commands inside the container, in\n",
    "order to install the SSH server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.execute(my_container.uuid, 'apt update')\n",
    "container.execute(my_container.uuid, 'apt -y install openssh-server')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more necessary step before we can access the container over\n",
    "SSH - we need to make sure our key is installed on the container. Here,\n",
    "we will upload the key from the Jupyter environment, and make sure it is\n",
    "configured with the appropriate file permissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp_keys\n",
    "!cp /work/.ssh/id_rsa.pub tmp_keys/authorized_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.execute(my_container.uuid, 'mkdir -p /root/.ssh')\n",
    "container.upload(my_container.uuid, \"./tmp_keys/authorized_keys\", \"/root/.ssh\")\n",
    "container.execute(my_container.uuid, 'chown root /root/.ssh')\n",
    "container.execute(my_container.uuid, 'chown root /root/.ssh/authorized_keys')\n",
    "container.execute(my_container.uuid, 'chmod go-w /root')\n",
    "container.execute(my_container.uuid, 'chmod 700 /root/.ssh')\n",
    "container.execute(my_container.uuid, 'chmod 600 /root/.ssh/authorized_keys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the SSH server in the container. The following cell should print\n",
    "“sshd is running”. It it’s not running, it can be an indication that the\n",
    "SSH server was not fully installed; wait a minute or two and then try\n",
    "this cell again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.execute(my_container.uuid, 'service ssh start')\n",
    "container.execute(my_container.uuid, 'service ssh status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can open a terminal in the Jupyter interface to access the\n",
    "container over SSH, using the SSH command that is printed by the\n",
    "following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ssh root@%s\" % public_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfering files to the container\n",
    "\n",
    "Later in this notebook, we’ll run an image classification model - a\n",
    "model that accepts an image as input and “predicts” the name of the\n",
    "object in the image - inside the container. To do this, we’ll need to\n",
    "upload some files to the container:\n",
    "\n",
    "-   an already-trained model\n",
    "-   a list of labels - this maps the integer values “predicted” by the\n",
    "    model to human readable object names\n",
    "-   a sample image\n",
    "-   and Python code to load the model and make a prediction on the image\n",
    "\n",
    "We first clone the `image_model` folder of\n",
    "[this](https://github.com/teaching-on-testbeds/edge-cpu-inference.git)\n",
    "git repository to bring the relevant files onto the Chameleon server.\n",
    "Run the following cell to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/teaching-on-testbeds/edge-cpu-inference.git\n",
    "!cd \n",
    "!cp -R ./edge-cpu-inference/image_model ./\n",
    "!rm -f -R edge-cpu-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all contained in the `image_model` directory. We can upload\n",
    "them to the container using the `container.upload` function, and specify\n",
    "the source directory (in the Jupyter environment) and destination\n",
    "directory (on the container).\n",
    "\n",
    "*Make sure to upload this directory even if you choose to test your\n",
    "custom model; This directory contains the inference script.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.upload(my_container.uuid, \"./image_model\", \"/root/\")\n",
    "container.execute(my_container.uuid, \"ls -h /root/image_model/\") #Check if all the files under the image_model directory have been uploaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload you custom pre-trained model, use the following cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First upload the model to the Chameleon: - Click on the folder icon on\n",
    "the left to see your storage, if it isn’t already open. Click on the\n",
    "image_model directory, we will upload our model under this directory.\n",
    "Now click on “Upload”. - Wait until all uploads have completed. To check\n",
    "if all the files have been uploaded successfully, check the size of your\n",
    "MODELFILE on the server using the `!ls -a image_model/<MODELFILENAME>`.\n",
    "If the size matches your file size, then proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.upload(my_container.uuid, \".image_model/<MODELFILE>\", \"/root/image_model/\") #Replace MODELFILE with the name of your model file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a pre-trained image classification model to do inference\n",
    "\n",
    "Now, we can use the model we uploaded to the container, and do\n",
    "inference - make a prediction - *on* the container.\n",
    "\n",
    "In this example, we will use a machine learning model that is\n",
    "specifically designed for inference on resource-constrained edge\n",
    "devices. In general, there are several strategies to reduce inference\n",
    "time on edge devices:\n",
    "\n",
    "-   **Model design**: models meant for inference on edge devices are\n",
    "    often designed specifically to reduce memory and/or inference time.\n",
    "    The model in this example is a MobileNet, which like many image\n",
    "    classification models uses a *convolution* operation to process its\n",
    "    input, but MobileNets use a kind of convolution that is much faster\n",
    "    and requires fewer operations than a “standard” convolution.\n",
    "-   **Model Compression**: another approach to faster inference on edge\n",
    "    devices is model compression, a group of techniques that try to\n",
    "    reduce the size of the model without affecting its accuracy. The\n",
    "    model in this example is a quantized model, which means that the\n",
    "    numeric parameters in the model are represented using fewer bits\n",
    "    than in a “standard” model. These quantized parameters can also be\n",
    "    processed using faster mathematical operations, potentially\n",
    "    improving the inference time.\n",
    "-   **Hardware Acceleration**: a third popular technique to improving\n",
    "    inference time at the edge is with hardware acceleration - using\n",
    "    specialized computer chips, GPUs, or TPUs that can perform the\n",
    "    operations involved in inference very fast. In this example, though,\n",
    "    we are going to use CPU-based inference, which means that there is\n",
    "    no hardware acceleration.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "> For more information about these strategies, see: J. Chen and X. Ran,\n",
    "> “Deep Learning With Edge Computing: A Review,” in Proceedings of the\n",
    "> IEEE, vol. 107, no. 8, pp. 1655-1674, Aug. 2019, doi:\n",
    "> 10.1109/JPROC.2019.2921977.\n",
    "> https://ieeexplore.ieee.org/document/8763885"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install a couple of Python libraries in the container:\n",
    "\n",
    "-   `tflite` is a library specifically designed for machine learning\n",
    "    inference on edge devices.\n",
    "-   `Pillow` is used for image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.execute(my_container.uuid, 'pip install tflite-runtime Pillow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can execute the machine learning model! We will ask it to make\n",
    "a prediction for the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('image_model/parrot.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run inference using the uploaded model. If you would like to run\n",
    "this inference using your custom model, replace the below command with\n",
    "the following:\n",
    "`container.execute(my_container.uuid, 'python /root/image_model/model.py --model <MODELFILE_PATH> --label <IMAGELABEL_PATH> --image <TESTIMAGE_PATH>')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = container.execute(my_container.uuid, 'python /root/image_model/model.py --model mobilenet_v2_1.0_224_quantized_1_default_1.tflite --label imagenet_labels.txt --image parrot.jpg ')\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note of the time it took to generate the prediction - would this\n",
    "inference time be acceptable for all applications? Also make a note of\n",
    "the model’s three best “guesses” regarding the label of the image - is\n",
    "the prediction accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the container\n",
    "\n",
    "Finally, we should stop and delete our container so that others can\n",
    "create new containers using the same lease. To delete our container, we\n",
    "can run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.destroy_container(my_container.uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also free up the IP that you we attached to the container, now that it\n",
    "is no longer in use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_details = chi.network.get_floating_ip(public_ip)\n",
    "chi.neutron().delete_floatingip(ip_details[\"id\"])"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}
